{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Project\n",
        "- ìž‘ì„±ìž: ë°•ì¸ì•  ðŸ€\n",
        "- ìž‘ì„±ë‚ ì§œ: 2024.06.01\n",
        "- ëª©ì : binary classification\n",
        "- ëª©í‘œ: F1-Scoreê°€ ë†’ê²Œ ë‚˜ì˜¤ëŠ” ì „ì²˜ë¦¬ ê¸°ë²• + ëª¨ë¸ ì¡°í•© ì°¾ê¸° ðŸŒŸ"
      ],
      "metadata": {
        "id": "ZyjUGJOcz0vS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcwN8bhvutWi"
      },
      "outputs": [],
      "source": [
        "for df in encoded_df_group:\n",
        "  df[\"y\"] = df[\"y\"].replace({\"no\": 0, \"yes\": 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fW7NR5J8O8Z"
      },
      "source": [
        "## Train-Valid Split\n",
        "- processed data set ì´ìš©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv6qVUk88Wdp"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "random_state = 100\n",
        "shuffle = True\n",
        "test_size_ratio = 0.25\n",
        "\n",
        "processed_train_df_group = []\n",
        "processed_valid_df_group = []\n",
        "\n",
        "for df in encoded_df_group:\n",
        "  processed_train_df, processed_valid_df = train_test_split(df, test_size=test_size_ratio, random_state=random_state, shuffle=shuffle)\n",
        "  processed_train_df_group.append(processed_valid_df)\n",
        "  processed_valid_df_group.append(processed_valid_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVo7bwwnZDoK"
      },
      "source": [
        "# ðŸ“Œ ë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•œ ì´ì§„ ë¶„ë¥˜",
        "- supervised learning ðŸŒŸ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A60mWJXo4pcP"
      },
      "source": [
        "#### Lab4\n",
        "- Logistic Regression\n",
        "- Polynomial Regression\n",
        "- Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRVoUoI7eEL_"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import KFold, cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "YPGKwxnHeGEj",
        "outputId": "aa110e99-0180-4d2a-c817-41a776ebbeba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nscores = cross_val_score(model, X_train, y_train, cv=kf, scoring=\"roc_auc\")\\n\\n# kê°œ samplesì˜ scoreë¥¼ ê°ê° ë°°ì—´(list)ë¡œ í™•ì¸\\nprint(\"Scores from each iteration:\", scores)\\n# kê°œ samplesì˜ scoreë¥¼ í‰ê· ë‚´ê¸°\\nprint(\"Average score:\", scores.mean())\\n'"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_iter = 10000\n",
        "\n",
        "# ëª¨ë“  ëª¨ë¸ë“¤ì„ dictionary í˜•íƒœë¡œ ë„£ì–´ì„œ ê°ê° í•™ìŠµ\n",
        "models = {\n",
        "  \"Baseline\": LogisticRegression(solver=\"saga\", max_iter=max_iter, penalty=None),\n",
        "  \"L2\": LogisticRegression(solver=\"saga\", max_iter=max_iter, penalty=\"l2\", C=1.0),\n",
        "  \"L1\": LogisticRegression(solver=\"saga\", max_iter=max_iter, penalty=\"l1\", C=1.0),\n",
        "  \"Polynomial\": Pipeline([(\"poly_features\", PolynomialFeatures(degree=2)),\n",
        "                          (\"softmax_reg\", LogisticRegression(solver=\"saga\", max_iter=max_iter, penalty=None))]),\n",
        "  \"DecisionTree\": DecisionTreeClassifier(criterion=\"gini\", max_depth=5, min_samples_split=2, min_impurity_decrease=0.0)\n",
        "}\n",
        "\n",
        "\n",
        "## ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í‰ê°€\n",
        "# n_splites = number of k  (default: 5)\n",
        "random_state = 100\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
        "\n",
        "# ë¨¸ì‹ ëŸ¬ë‹ìœ¼ë¡œ ë¶„ë¥˜í•œ kê°œì˜ samplesì— ëŒ€í•´ì„œ scoreë¥¼ í™•ì¸í•˜ê¸°\n",
        "# cross validation ë°©ì‹ìœ¼ë¡œ train setì„ ê°€ì§€ê³  ëª¨ë¸ì„ í‰ê°€í•˜ê¸°\n",
        "# scoringì€ 5ê°€ì§€ ë°©ì‹\n",
        "'''\n",
        "1) accuracy: Accuracy (default)\n",
        "2) roc_auc: Area under the receiver operating characteristic (ROC) curve\n",
        "3) f1: F1 score\n",
        "4) precision: Precision\n",
        "5) recall: Recal\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IGGShQPveBy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tknHL7YOlGmV",
        "outputId": "d52be279-c147-4bb3-e289-36aede745130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "104\n"
          ]
        }
      ],
      "source": [
        "print(len(encoded_df_group))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht7mC72k9Fkp"
      },
      "source": [
        "- ì¼ë‹¨ sample í•˜ë‚˜ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸\n",
        "   - ìž˜ ëŒì•„ê° í™•ì¸ ì™„ë£Œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMMs-EhzArYb"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "df = processed_train_df_group[5]\n",
        "X_train = df.drop('y', axis=1, inplace=False).values\n",
        "y_train = df['y'].values\n",
        "\n",
        "# Validation\n",
        "X_valid = processed_valid_df_group[5].drop('y', axis=1, inplace=False).values\n",
        "y_valid = processed_valid_df_group[5]['y'].values\n",
        "\n",
        "# ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\n",
        "for name, model in models.items():\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  print(f'Model: {name}')\n",
        "\n",
        "  y_prob = model.predict_proba(X_valid)\n",
        "  y_cls = model.predict(X_valid)\n",
        "\n",
        "  # 3ê°€ì§€ í‰ê°€ ì§€í‘œ (evalution metrics)\n",
        "  print(\"Accuracy:\", accuracy_score(y_valid, y_cls))\n",
        "  print(\"F1:\", f1_score(y_valid, y_cls))\n",
        "  print(\"ROC AUC:\", roc_auc_score(y_valid, y_prob[:, 1]))\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCecrh7llGmW",
        "outputId": "3fce5bd2-86fc-4831-f160-2e65ec4cb357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: Baseline, 0-th data, F1: 0.32676518883415434\n",
            "Model: L2, 0-th data, F1: 0.32676518883415434\n",
            "Model: L1, 0-th data, F1: 0.32676518883415434\n",
            "Model: Polynomial, 0-th data, F1: 0.32418952618453867\n",
            "Model: DecisionTree, 0-th data, F1: 0.4319880418535127\n",
            "Model: Baseline, 1-th data, F1: 0.32676518883415434\n",
            "Model: L2, 1-th data, F1: 0.32676518883415434\n",
            "Model: L1, 1-th data, F1: 0.32676518883415434\n",
            "Model: Polynomial, 1-th data, F1: 0.32418952618453867\n",
            "Model: DecisionTree, 1-th data, F1: 0.4319880418535127\n",
            "Model: Baseline, 2-th data, F1: 0.32676518883415434\n",
            "Model: L2, 2-th data, F1: 0.32676518883415434\n",
            "Model: L1, 2-th data, F1: 0.32676518883415434\n",
            "Model: Polynomial, 2-th data, F1: 0.32418952618453867\n",
            "Model: DecisionTree, 2-th data, F1: 0.4319880418535127\n",
            "Model: Baseline, 3-th data, F1: 0.32676518883415434\n",
            "Model: L2, 3-th data, F1: 0.32676518883415434\n",
            "Model: L1, 3-th data, F1: 0.32676518883415434\n",
            "Model: Polynomial, 3-th data, F1: 0.32418952618453867\n",
            "Model: DecisionTree, 3-th data, F1: 0.4319880418535127\n",
            "Model: Baseline, 4-th data, F1: 0.32676518883415434\n",
            "Model: L2, 4-th data, F1: 0.32676518883415434\n",
            "Model: L1, 4-th data, F1: 0.32676518883415434\n",
            "Model: Polynomial, 4-th data, F1: 0.32418952618453867\n",
            "Model: DecisionTree, 4-th data, F1: 0.4319880418535127\n",
            "Model: Baseline, 5-th data, F1: 0.32676518883415434\n",
            "Model: L2, 5-th data, F1: 0.32676518883415434\n",
            "Model: L1, 5-th data, F1: 0.32676518883415434\n"
          ]
        }
      ],
      "source": [
        "# ëª¨ë“  ì „ì²˜ë¦¬ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ\n",
        "for i in range(6, len(encoded_df_group)):\n",
        "  # Train\n",
        "  df = processed_train_df_group[i]\n",
        "  X_train = df.drop('y', axis=1, inplace=False).values\n",
        "  y_train = df['y'].values\n",
        "\n",
        "  # Validation\n",
        "  X_valid = processed_valid_df_group[i].drop('y', axis=1, inplace=False).values\n",
        "  y_valid = processed_valid_df_group[i]['y'].values\n",
        "\n",
        "\n",
        "\n",
        "  # forë¬¸ì„ ì´ìš©í•´ ê°ê°ì˜ ëª¨ë¸ì— ëŒ€í•´ ëª¨ë¸ í‰ê°€ ìˆ˜í–‰ ë° ROC ê°’ì„ í”„ë¦°íŠ¸í•´ì„œ ì„±ëŠ¥ ë¹„êµ\n",
        "  # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\n",
        "  for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "    y_prob = model.predict_proba(X_valid)\n",
        "    y_cls = model.predict(X_valid)\n",
        "\n",
        "    # 3ê°€ì§€ í‰ê°€ ì§€í‘œ (evalution metrics)\n",
        "    print(f'Model: {name}, {i}-th data, F1: {f1_score(y_valid, y_cls)}')\n",
        "    '''\n",
        "    print(\"Accuracy:\", accuracy_score(y_valid, y_cls))\n",
        "    print(\"F1:\", f1_score(y_valid, y_cls))\n",
        "    print(\"ROC AUC:\", roc_auc_score(y_valid, y_prob[:, 1]))\n",
        "    print()\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT3bBKmtqzd4",
        "outputId": "19e6a676-f63d-4d27-9cfd-01a7a43c20a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: Baseline, 90-th data, F1: 0.0\n",
            "Model: L2, 90-th data, F1: 0.0\n",
            "Model: L1, 90-th data, F1: 0.0\n",
            "Model: Polynomial, 90-th data, F1: 0.0\n",
            "Model: DecisionTree, 90-th data, F1: 0.3082077051926298\n",
            "Model: Baseline, 91-th data, F1: 0.0\n",
            "Model: L2, 91-th data, F1: 0.0\n",
            "Model: L1, 91-th data, F1: 0.0\n",
            "Model: Polynomial, 91-th data, F1: 0.0\n",
            "Model: DecisionTree, 91-th data, F1: 0.3082077051926298\n",
            "Model: Baseline, 92-th data, F1: 0.0\n",
            "Model: L2, 92-th data, F1: 0.0\n",
            "Model: L1, 92-th data, F1: 0.0\n",
            "Model: Polynomial, 92-th data, F1: 0.0\n",
            "Model: DecisionTree, 92-th data, F1: 0.3082077051926298\n",
            "Model: Baseline, 93-th data, F1: 0.0\n",
            "Model: L2, 93-th data, F1: 0.0\n",
            "Model: L1, 93-th data, F1: 0.0\n",
            "Model: Polynomial, 93-th data, F1: 0.0\n",
            "Model: DecisionTree, 93-th data, F1: 0.3082077051926298\n",
            "Model: Baseline, 94-th data, F1: 0.0\n",
            "Model: L2, 94-th data, F1: 0.0\n",
            "Model: L1, 94-th data, F1: 0.0\n",
            "Model: Polynomial, 94-th data, F1: 0.0\n",
            "Model: DecisionTree, 94-th data, F1: 0.3082077051926298\n",
            "Model: Baseline, 95-th data, F1: 0.0\n",
            "Model: L2, 95-th data, F1: 0.0\n",
            "Model: L1, 95-th data, F1: 0.0\n",
            "Model: Polynomial, 95-th data, F1: 0.0\n",
            "Model: DecisionTree, 95-th data, F1: 0.3082077051926298\n",
            "Model: Baseline, 96-th data, F1: 0.047619047619047616\n",
            "Model: L2, 96-th data, F1: 0.047619047619047616\n",
            "Model: L1, 96-th data, F1: 0.047619047619047616\n",
            "Model: Polynomial, 96-th data, F1: 0.0\n",
            "Model: DecisionTree, 96-th data, F1: 0.6785714285714285\n",
            "Model: Baseline, 97-th data, F1: 0.047619047619047616\n",
            "Model: L2, 97-th data, F1: 0.047619047619047616\n",
            "Model: L1, 97-th data, F1: 0.047619047619047616\n",
            "Model: Polynomial, 97-th data, F1: 0.0\n",
            "Model: DecisionTree, 97-th data, F1: 0.6545454545454547\n",
            "Model: Baseline, 98-th data, F1: 0.047619047619047616\n",
            "Model: L2, 98-th data, F1: 0.047619047619047616\n",
            "Model: L1, 98-th data, F1: 0.047619047619047616\n",
            "Model: Polynomial, 98-th data, F1: 0.0\n",
            "Model: DecisionTree, 98-th data, F1: 0.6785714285714285\n",
            "Model: Baseline, 99-th data, F1: 0.047619047619047616\n",
            "Model: L2, 99-th data, F1: 0.047619047619047616\n",
            "Model: L1, 99-th data, F1: 0.047619047619047616\n",
            "Model: Polynomial, 99-th data, F1: 0.0\n",
            "Model: DecisionTree, 99-th data, F1: 0.6785714285714285\n",
            "Model: Baseline, 100-th data, F1: 0.047619047619047616\n",
            "Model: L2, 100-th data, F1: 0.047619047619047616\n",
            "Model: L1, 100-th data, F1: 0.047619047619047616\n",
            "Model: Polynomial, 100-th data, F1: 0.0\n",
            "Model: DecisionTree, 100-th data, F1: 0.6785714285714285\n",
            "Model: Baseline, 101-th data, F1: 0.047619047619047616\n",
            "Model: L2, 101-th data, F1: 0.047619047619047616\n",
            "Model: L1, 101-th data, F1: 0.047619047619047616\n",
            "Model: Polynomial, 101-th data, F1: 0.0\n",
            "Model: DecisionTree, 101-th data, F1: 0.6545454545454547\n",
            "Model: Baseline, 102-th data, F1: 0.047619047619047616\n",
            "Model: L2, 102-th data, F1: 0.047619047619047616\n",
            "Model: L1, 102-th data, F1: 0.047619047619047616\n",
            "Model: Polynomial, 102-th data, F1: 0.0\n",
            "Model: DecisionTree, 102-th data, F1: 0.6785714285714285\n",
            "Model: Baseline, 103-th data, F1: 0.047619047619047616\n",
            "Model: L2, 103-th data, F1: 0.047619047619047616\n",
            "Model: L1, 103-th data, F1: 0.047619047619047616\n",
            "Model: Polynomial, 103-th data, F1: 0.0\n",
            "Model: DecisionTree, 103-th data, F1: 0.6545454545454547\n"
          ]
        }
      ],
      "source": [
        "for i in range(90, len(encoded_df_group)):\n",
        "  # Train\n",
        "  df = processed_train_df_group[i]\n",
        "  X_train = df.drop('y', axis=1, inplace=False).values\n",
        "  y_train = df['y'].values\n",
        "\n",
        "  # Validation\n",
        "  X_valid = processed_valid_df_group[i].drop('y', axis=1, inplace=False).values\n",
        "  y_valid = processed_valid_df_group[i]['y'].values\n",
        "\n",
        "\n",
        "  for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "    y_prob = model.predict_proba(X_valid)\n",
        "    y_cls = model.predict(X_valid)\n",
        "\n",
        "    print(f'Model: {name}, {i}-th data, F1: {f1_score(y_valid, y_cls)}')\n",
        "    '''\n",
        "    print(\"Accuracy:\", accuracy_score(y_valid, y_cls))\n",
        "    print(\"F1:\", f1_score(y_valid, y_cls))\n",
        "    print(\"ROC AUC:\", roc_auc_score(y_valid, y_prob[:, 1]))\n",
        "    print()\n",
        "    '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htp1DyrHj2ZI"
      },
      "source": [
        "```\n",
        "[ê²°ê³¼]\n",
        "Model: Baseline, 30-th data, F1: 0.32676518883415434\n",
        "Model: L2, 30-th data, F1: 0.32676518883415434\n",
        "Model: L1, 30-th data, F1: 0.32676518883415434\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI8YIALvPam9"
      },
      "source": [
        "```\n",
        "[ê²°ê³¼]\n",
        "90th - 103th\n",
        "odel: Baseline, 90-th data, F1: 0.0\n",
        "Model: L2, 90-th data, F1: 0.0\n",
        "Model: L1, 90-th data, F1: 0.0\n",
        "Model: Polynomial, 90-th data, F1: 0.0\n",
        "Model: DecisionTree, 90-th data, F1: 0.3082077051926298\n",
        "Model: Baseline, 91-th data, F1: 0.0\n",
        "Model: L2, 91-th data, F1: 0.0\n",
        "Model: L1, 91-th data, F1: 0.0\n",
        "Model: Polynomial, 91-th data, F1: 0.0\n",
        "Model: DecisionTree, 91-th data, F1: 0.3082077051926298\n",
        "Model: Baseline, 92-th data, F1: 0.0\n",
        "Model: L2, 92-th data, F1: 0.0\n",
        "Model: L1, 92-th data, F1: 0.0\n",
        "Model: Polynomial, 92-th data, F1: 0.0\n",
        "Model: DecisionTree, 92-th data, F1: 0.3082077051926298\n",
        "Model: Baseline, 93-th data, F1: 0.0\n",
        "Model: L2, 93-th data, F1: 0.0\n",
        "Model: L1, 93-th data, F1: 0.0\n",
        "Model: Polynomial, 93-th data, F1: 0.0\n",
        "Model: DecisionTree, 93-th data, F1: 0.3082077051926298\n",
        "Model: Baseline, 94-th data, F1: 0.0\n",
        "Model: L2, 94-th data, F1: 0.0\n",
        "Model: L1, 94-th data, F1: 0.0\n",
        "Model: Polynomial, 94-th data, F1: 0.0\n",
        "Model: DecisionTree, 94-th data, F1: 0.3082077051926298\n",
        "Model: Baseline, 95-th data, F1: 0.0\n",
        "Model: L2, 95-th data, F1: 0.0\n",
        "Model: L1, 95-th data, F1: 0.0\n",
        "Model: Polynomial, 95-th data, F1: 0.0\n",
        "Model: DecisionTree, 95-th data, F1: 0.3082077051926298\n",
        "Model: Baseline, 96-th data, F1: 0.047619047619047616\n",
        "Model: L2, 96-th data, F1: 0.047619047619047616\n",
        "Model: L1, 96-th data, F1: 0.047619047619047616\n",
        "Model: Polynomial, 96-th data, F1: 0.0\n",
        "Model: DecisionTree, 96-th data, F1: 0.6785714285714285\n",
        "Model: Baseline, 97-th data, F1: 0.047619047619047616\n",
        "Model: L2, 97-th data, F1: 0.047619047619047616\n",
        "Model: L1, 97-th data, F1: 0.047619047619047616\n",
        "Model: Polynomial, 97-th data, F1: 0.0\n",
        "Model: DecisionTree, 97-th data, F1: 0.6545454545454547\n",
        "Model: Baseline, 98-th data, F1: 0.047619047619047616\n",
        "Model: L2, 98-th data, F1: 0.047619047619047616\n",
        "Model: L1, 98-th data, F1: 0.047619047619047616\n",
        "Model: Polynomial, 98-th data, F1: 0.0\n",
        "Model: DecisionTree, 98-th data, F1: 0.6785714285714285\n",
        "Model: Baseline, 99-th data, F1: 0.047619047619047616\n",
        "Model: L2, 99-th data, F1: 0.047619047619047616\n",
        "Model: L1, 99-th data, F1: 0.047619047619047616\n",
        "Model: Polynomial, 99-th data, F1: 0.0\n",
        "Model: DecisionTree, 99-th data, F1: 0.6785714285714285\n",
        "Model: Baseline, 100-th data, F1: 0.047619047619047616\n",
        "Model: L2, 100-th data, F1: 0.047619047619047616\n",
        "Model: L1, 100-th data, F1: 0.047619047619047616\n",
        "Model: Polynomial, 100-th data, F1: 0.0\n",
        "Model: DecisionTree, 100-th data, F1: 0.6785714285714285\n",
        "Model: Baseline, 101-th data, F1: 0.047619047619047616\n",
        "Model: L2, 101-th data, F1: 0.047619047619047616\n",
        "Model: L1, 101-th data, F1: 0.047619047619047616\n",
        "Model: Polynomial, 101-th data, F1: 0.0\n",
        "Model: DecisionTree, 101-th data, F1: 0.6545454545454547\n",
        "Model: Baseline, 102-th data, F1: 0.047619047619047616\n",
        "Model: L2, 102-th data, F1: 0.047619047619047616\n",
        "Model: L1, 102-th data, F1: 0.047619047619047616\n",
        "Model: Polynomial, 102-th data, F1: 0.0\n",
        "Model: DecisionTree, 102-th data, F1: 0.6785714285714285\n",
        "Model: Baseline, 103-th data, F1: 0.047619047619047616\n",
        "Model: L2, 103-th data, F1: 0.047619047619047616\n",
        "Model: L1, 103-th data, F1: 0.047619047619047616\n",
        "Model: Polynomial, 103-th data, F1: 0.0\n",
        "Model: DecisionTree, 103-th data, F1: 0.6545454545454547\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7smN_OpkACD",
        "outputId": "c015409d-8b0c-4bd9-fddd-d5e178e21711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 6.77884429\n",
            "Iteration 2, loss = 3.77146987\n",
            "Iteration 3, loss = 3.63350087\n",
            "Iteration 4, loss = 3.58931429\n",
            "Iteration 5, loss = 3.98281644\n",
            "Iteration 6, loss = 3.38896534\n",
            "Iteration 7, loss = 2.37042087\n",
            "Iteration 8, loss = 3.13216584\n",
            "Iteration 9, loss = 2.38938907\n",
            "Iteration 10, loss = 2.38910141\n",
            "Iteration 11, loss = 1.74714135\n",
            "Iteration 12, loss = 1.52783845\n",
            "Iteration 13, loss = 2.49791048\n",
            "Iteration 14, loss = 1.51129688\n",
            "Iteration 15, loss = 1.24739805\n",
            "Iteration 16, loss = 1.04036795\n",
            "Iteration 17, loss = 0.66481931\n",
            "Iteration 18, loss = 0.81678280\n",
            "Iteration 19, loss = 0.79415071\n",
            "Iteration 20, loss = 0.62368814\n",
            "Iteration 21, loss = 0.57667125\n",
            "Iteration 22, loss = 0.46404389\n",
            "Iteration 23, loss = 0.38727159\n",
            "Iteration 24, loss = 0.46836977\n",
            "Iteration 25, loss = 0.53286497\n",
            "Iteration 26, loss = 0.41824085\n",
            "Iteration 27, loss = 0.51313425\n",
            "Iteration 28, loss = 0.31095144\n",
            "Iteration 29, loss = 0.37891513\n",
            "Iteration 30, loss = 0.41891456\n",
            "Iteration 31, loss = 0.37814555\n",
            "Iteration 32, loss = 0.50295022\n",
            "Iteration 33, loss = 0.37327895\n",
            "Iteration 34, loss = 0.36621030\n",
            "Iteration 35, loss = 0.42726079\n",
            "Iteration 36, loss = 0.34296544\n",
            "Iteration 37, loss = 0.30880442\n",
            "Iteration 38, loss = 0.31341071\n",
            "Iteration 39, loss = 0.29228525\n",
            "Iteration 40, loss = 0.32912178\n",
            "Iteration 41, loss = 0.33372142\n",
            "Iteration 42, loss = 0.31130755\n",
            "Iteration 43, loss = 0.31227022\n",
            "Iteration 44, loss = 0.29833485\n",
            "Iteration 45, loss = 0.28987358\n",
            "Iteration 46, loss = 0.39877395\n",
            "Iteration 47, loss = 0.35061026\n",
            "Iteration 48, loss = 0.34465769\n",
            "Iteration 49, loss = 0.30124682\n",
            "Iteration 50, loss = 0.33645228\n",
            "Iteration 51, loss = 0.34536222\n",
            "Iteration 52, loss = 0.30071973\n",
            "Iteration 53, loss = 0.29952949\n",
            "Iteration 54, loss = 0.32409516\n",
            "Iteration 55, loss = 0.33594760\n",
            "Iteration 56, loss = 0.30208882\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "model = MLPClassifier(hidden_layer_sizes=(50, 30),\n",
        "                      max_iter=300,\n",
        "                      activation='relu',\n",
        "                      solver='adam',\n",
        "                      batch_size=200,\n",
        "                      learning_rate='invscaling',\n",
        "                      learning_rate_init=0.01,\n",
        "                      power_t=0.5,  # Exponent for inverse scaling learning rate\n",
        "                      warm_start=True,\n",
        "                      random_state=100,\n",
        "                      verbose=True) # Enable verbose to monitor\n",
        "\n",
        "\n",
        "df = processed_train_df_group[24]\n",
        "X_train = df.drop('y', axis=1, inplace=False).values\n",
        "y_train = df['y'].values\n",
        "\n",
        "# Validation\n",
        "X_valid = processed_valid_df_group[24].drop('y', axis=1, inplace=False).values\n",
        "y_valid = processed_valid_df_group[24]['y'].values\n",
        "\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train)\n",
        "y_prob = model.predict_proba(X_valid)\n",
        "y_cls = model.predict(X_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmDUgOhocn3i"
      },
      "source": [
        "#### Lab5. Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Njfyp2tmcm1J"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "from sklearn.model_selection import KFold, GridSearchCV\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOxj8YNBdaVV"
      },
      "outputs": [],
      "source": [
        "# ì„±ëŠ¥ ì¸¡ì • ë°©ì‹ì€ Negative MSEë¡œ í•  ê²ƒìž„\n",
        "# ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ í•´ë„ ë¨\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
        "scoring = \"neg_mean_squared_error\"\n",
        "\n",
        "models = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj-WoxOTxtwP"
      },
      "source": [
        "- í•™ìŠµ ì‹œìž‘"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4TESAP7lGmp"
      },
      "outputs": [],
      "source": [
        "def run_models(i):\n",
        "  # Train\n",
        "  df = processed_train_df_group[i]\n",
        "  X_train = df.drop('y', axis=1, inplace=False).values\n",
        "  y_train = df['y'].values\n",
        "\n",
        "  # Validation\n",
        "  X_valid = processed_valid_df_group[i].drop('y', axis=1, inplace=False).values\n",
        "  y_valid = processed_valid_df_group[i]['y'].values\n",
        "\n",
        "  #\n",
        "  model = DecisionTreeRegressor(random_state=random_state)\n",
        "  param_grid = {\n",
        "      \"max_depth\": [5, 10, 20],\n",
        "      \"min_samples_split\": [2, 10, 20],\n",
        "      \"ccp_alpha\": [0.0, 0.01],\n",
        "  }\n",
        "  grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit=True, n_jobs=-1)\n",
        "  grid_search.fit(X_train, y_train)\n",
        "  y_pred = grid_search.predict(X_valid)\n",
        "  # y_predë¥¼ 0ê³¼ 1ë¡œ ì´ì§„ ë¶„ë¥˜\n",
        "  y_cls = np.where(y_pred > 0.5, 1, 0)\n",
        "  print(f'name: Decision Tree, {i}-th, F1-Score: {f1_score(y_valid, y_cls)}')\n",
        "\n",
        "\n",
        "  # Bagging\n",
        "  base_model = DecisionTreeRegressor()\n",
        "  model = BaggingRegressor(estimator=base_model,\n",
        "                          bootstrap=True,\n",
        "                          n_jobs=-1,\n",
        "                          random_state=random_state)\n",
        "  param_grid = {\n",
        "      \"n_estimators\": [25, 50]\n",
        "  }\n",
        "  grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit=True, n_jobs=-1)\n",
        "  grid_search.fit(X_train, y_train)\n",
        "  y_pred = grid_search.predict(X_valid)\n",
        "  # y_predë¥¼ 0ê³¼ 1ë¡œ ì´ì§„ ë¶„ë¥˜\n",
        "  y_cls = np.where(y_pred > 0.5, 1, 0)\n",
        "  print(f'name: Bagging, {i}-th, F1-Score: {f1_score(y_valid, y_cls)}')\n",
        "\n",
        "\n",
        "  #\n",
        "  model = RandomForestRegressor(max_depth=None,\n",
        "                                min_samples_split=2,\n",
        "                                bootstrap=True,\n",
        "                                n_jobs=-1,\n",
        "                                random_state=random_state)\n",
        "  param_grid = {\n",
        "      \"n_estimators\": [25, 50],\n",
        "      \"max_features\": [0.5, \"sqrt\", \"log2\", None],\n",
        "  }\n",
        "  grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit=True, n_jobs=-1)\n",
        "  grid_search.fit(X_train, y_train)\n",
        "  y_pred = grid_search.predict(X_valid)\n",
        "  # y_predë¥¼ 0ê³¼ 1ë¡œ ì´ì§„ ë¶„ë¥˜\n",
        "  y_cls = np.where(y_pred > 0.5, 1, 0)\n",
        "  print(f'name: Random Forest, {i}-th, F1-Score: {f1_score(y_valid, y_cls)}')\n",
        "\n",
        "  #\n",
        "  model = AdaBoostRegressor(loss=\"linear\",\n",
        "                          random_state=random_state)\n",
        "  param_grid = {\n",
        "      \"n_estimators\": [25, 50],\n",
        "      \"estimator\": [DecisionTreeRegressor(max_depth=3), DecisionTreeRegressor(max_depth=6)],\n",
        "      \"learning_rate\": [0.1, 1.0],\n",
        "  }\n",
        "  grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit=True, n_jobs=-1)\n",
        "  grid_search.fit(X_train, y_train)\n",
        "  y_pred = grid_search.predict(X_valid)\n",
        "  # y_predë¥¼ 0ê³¼ 1ë¡œ ì´ì§„ ë¶„ë¥˜\n",
        "  y_cls = np.where(y_pred > 0.5, 1, 0)\n",
        "  print(f'name: AdaBoost, {i}-th, F1-Score: {f1_score(y_valid, y_cls)}')\n",
        "\n",
        "\n",
        "  #\n",
        "  model = GradientBoostingRegressor(loss=\"squared_error\",\n",
        "                                  subsample=1.0,\n",
        "                                  random_state=random_state)\n",
        "  param_grid = {\n",
        "      \"n_estimators\": [25, 50],\n",
        "      \"max_depth\": [3, 6],\n",
        "      \"learning_rate\": [0.0, 0.1],\n",
        "  }\n",
        "  grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit=True, n_jobs=-1)\n",
        "  grid_search.fit(X_train, y_train)\n",
        "  y_pred = grid_search.predict(X_valid)\n",
        "  # y_predë¥¼ 0ê³¼ 1ë¡œ ì´ì§„ ë¶„ë¥˜\n",
        "  y_cls = np.where(y_pred > 0.5, 1, 0)\n",
        "  print(f'name: Gradient Boosting, {i}-th, F1-Score: {f1_score(y_valid, y_cls)}')\n",
        "\n",
        "\n",
        "  #\n",
        "  model = XGBRegressor(subsample=1.0,\n",
        "                     learning_rate=0.1,\n",
        "                     max_depth=6,\n",
        "                     n_jobs=-1,\n",
        "                     random_state=random_state)\n",
        "  param_grid = {\n",
        "      \"n_estimators\": [25, 50],\n",
        "      \"reg_alpha\": [0, 0.1],\n",
        "      \"reg_lambda\": [0, 0.1],\n",
        "  }\n",
        "  grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit=True)\n",
        "  grid_search.fit(X_train, y_train)\n",
        "  y_pred = grid_search.predict(X_valid)\n",
        "  # y_predë¥¼ 0ê³¼ 1ë¡œ ì´ì§„ ë¶„ë¥˜\n",
        "  y_cls = np.where(y_pred > 0.5, 1, 0)\n",
        "  print(f'name: XGBoost, {i}-th, F1-Score: {f1_score(y_valid, y_cls)}')\n",
        "\n",
        "\n",
        "  #\n",
        "  model = LGBMRegressor(learning_rate=0.1,\n",
        "                      data_sample_strategy=\"goss\",\n",
        "                      top_rate=0.2,\n",
        "                      other_rate=0.1,\n",
        "                      force_col_wise=True,\n",
        "                      verbosity=0,\n",
        "                      n_jobs=-1,\n",
        "                      random_state=random_state)\n",
        "  param_grid = {\n",
        "      \"n_estimators\": [25, 50],\n",
        "      \"reg_alpha\": [0, 0.1],\n",
        "      \"reg_lambda\": [0, 0.1],\n",
        "      \"enable_bundle\": [True, False]\n",
        "  }\n",
        "  grid_search = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit=True)\n",
        "  grid_search.fit(X_train, y_train)\n",
        "  y_pred = grid_search.predict(X_valid)\n",
        "  # y_predë¥¼ 0ê³¼ 1ë¡œ ì´ì§„ ë¶„ë¥˜\n",
        "  y_cls = np.where(y_pred > 0.5, 1, 0)\n",
        "  print(f'name: LightGBM, {i}-th, F1-Score: {f1_score(y_valid, y_cls)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "qmoAGJEeztGk",
        "outputId": "0ee5d948-f409-4947-85bd-6c3e1776f591"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name: Decision Tree, 30-th, F1-Score: 0.4498918529199712\n",
            "name: Bagging, 30-th, F1-Score: 0.996201844818231\n",
            "name: Random Forest, 30-th, F1-Score: 0.9956568946796959\n",
            "name: AdaBoost, 30-th, F1-Score: 0.3462132921174652\n",
            "name: Gradient Boosting, 30-th, F1-Score: 0.42289348171701113\n",
            "name: XGBoost, 30-th, F1-Score: 0.5093167701863354\n",
            "name: LightGBM, 30-th, F1-Score: 0.4604651162790698\n",
            "name: Decision Tree, 31-th, F1-Score: 0.4498918529199712\n",
            "name: Bagging, 31-th, F1-Score: 0.996201844818231\n",
            "name: Random Forest, 31-th, F1-Score: 0.9956568946796959\n",
            "name: AdaBoost, 31-th, F1-Score: 0.3462132921174652\n",
            "name: Gradient Boosting, 31-th, F1-Score: 0.42289348171701113\n",
            "name: XGBoost, 31-th, F1-Score: 0.5093167701863354\n",
            "name: LightGBM, 31-th, F1-Score: 0.4604651162790698\n",
            "name: Decision Tree, 32-th, F1-Score: 0.44992743105950656\n",
            "name: Bagging, 32-th, F1-Score: 0.9967462039045553\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-9e0533251e40>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_train_df_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mrun_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-55-262a4dffc357>\u001b[0m in \u001b[0;36mrun_models\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m     52\u001b[0m   }\n\u001b[1;32m     53\u001b[0m   \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m   \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m   \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;31m# y_predë¥¼ 0ê³¼ 1ë¡œ ì´ì§„ ë¶„ë¥˜\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for i in range(30, len(processed_train_df_group)):\n",
        "  run_models(i)\n",
        "'''\n",
        "[ê²°ê³¼]\n",
        "name: Decision Tree, 30-th, F1-Score: 0.4498918529199712\n",
        "name: Bagging, 30-th, F1-Score: 0.996201844818231\n",
        "name: Random Forest, 30-th, F1-Score: 0.9956568946796959\n",
        "name: AdaBoost, 30-th, F1-Score: 0.3462132921174652\n",
        "name: Gradient Boosting, 30-th, F1-Score: 0.42289348171701113\n",
        "name: XGBoost, 30-th, F1-Score: 0.5093167701863354\n",
        "name: LightGBM, 30-th, F1-Score: 0.4604651162790698\n",
        "name: Decision Tree, 31-th, F1-Score: 0.4498918529199712\n",
        "name: Bagging, 31-th, F1-Score: 0.996201844818231\n",
        "name: Random Forest, 31-th, F1-Score: 0.9956568946796959\n",
        "name: AdaBoost, 31-th, F1-Score: 0.3462132921174652\n",
        "name: Gradient Boosting, 31-th, F1-Score: 0.42289348171701113\n",
        "name: XGBoost, 31-th, F1-Score: 0.5093167701863354\n",
        "name: LightGBM, 31-th, F1-Score: 0.4604651162790698\n",
        "name: Decision Tree, 32-th, F1-Score: 0.44992743105950656\n",
        "name: Bagging, 32-th, F1-Score: 0.9967462039045553\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GbFUSB6oNvoR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
